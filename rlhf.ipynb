{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7n_KdYNJWMMs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "editable": true,
        "executionInfo": {
          "elapsed": 17891,
          "status": "ok",
          "timestamp": 1701917437255,
          "user": {
            "displayName": "maximo rulli",
            "userId": "05485638369195676564"
          },
          "user_tz": 180
        },
        "id": "7n_KdYNJWMMs",
        "outputId": "d916a8fc-1cdb-4220-f795-dc07e30e753b",
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "70192576-b94e-430a-9509-92a38dabe891",
      "metadata": {
        "executionInfo": {
          "elapsed": 20242,
          "status": "ok",
          "timestamp": 1701917457493,
          "user": {
            "displayName": "maximo rulli",
            "userId": "05485638369195676564"
          },
          "user_tz": 180
        },
        "id": "70192576-b94e-430a-9509-92a38dabe891"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/maximo/miniconda3/envs/cnc/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "import pandas as pd\n",
        "from transformers import (AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoConfig)\n",
        "import evaluate\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import numpy as np\n",
        "from utils import PROD_TOK, AUX_TOK, RewardModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6930d8dc-d087-441a-b42e-47c193a1aa81",
      "metadata": {
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1701917457494,
          "user": {
            "displayName": "maximo rulli",
            "userId": "05485638369195676564"
          },
          "user_tz": 180
        },
        "id": "6930d8dc-d087-441a-b42e-47c193a1aa81"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "id": "93d34f58-5576-4546-9844-81fc0ad68ebe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0,
          "referenced_widgets": [
            "19b3a2fcb1c145c5bcca348594ae1792",
            "cac8085184e64f4b9bc09299ab74e0e9",
            "9352dc55e25e4a7cac944623c0d6547a",
            "bafb7c930e454eb383f5b3d75f1baf3a",
            "b0c04d95fc6f4668ac05ec8c226cb63d",
            "73627d135a714041afff7e83c27f5d53",
            "e17e399f5f3d457ba9b413a57fc4cfa4",
            "784c567285104bb083b318af85f6eb2e",
            "581a3e619f974bc3923bf50de8ec9ad9",
            "a07ffb55fceb44ac92d6444affa72527",
            "df0c8a832dc0474db530ada319f19d5f"
          ]
        },
        "executionInfo": {
          "elapsed": 14484,
          "status": "ok",
          "timestamp": 1701917472496,
          "user": {
            "displayName": "maximo rulli",
            "userId": "05485638369195676564"
          },
          "user_tz": 180
        },
        "id": "93d34f58-5576-4546-9844-81fc0ad68ebe",
        "outputId": "b53263a6-14d5-43d8-868e-159e53067c7b",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(21, 768, padding_idx=20)\n",
              "      (position_embeddings): Embedding(1024, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 108,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#We initialize our custom reward model\n",
        "rm = RewardModel(\"VCNC/bert_piezas3\", hidden_size=768, classes=20).to(device)\n",
        "rm.load_state_dict(torch.load('rm_dict.pt'))\n",
        "rm.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "id": "5f1ee167-b83b-483c-af7e-c768c4eeb320",
      "metadata": {
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(21, 768, padding_idx=13)\n",
              "      (position_embeddings): Embedding(1024, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=20, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#We initialize 2 BERT pieces models, one to train and the other for reference\n",
        "model_new = AutoModelForSequenceClassification.from_pretrained(\"VCNC/bert_piezas3\").to(device)\n",
        "model_ref = AutoModelForSequenceClassification.from_pretrained(\"VCNC/bert_piezas3\").to(device)\n",
        "model_new.train()\n",
        "model_ref.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "id": "8bf1ba4d-a0c2-4fd6-bac1-f17f8a815b77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 4269,
          "status": "ok",
          "timestamp": 1701917476763,
          "user": {
            "displayName": "maximo rulli",
            "userId": "05485638369195676564"
          },
          "user_tz": 180
        },
        "id": "8bf1ba4d-a0c2-4fd6-bac1-f17f8a815b77",
        "outputId": "5d7bc78b-6ca4-49a7-c141-6bf5119d36b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reward model's output: SequenceClassifierOutput(loss=None, logits=tensor([[-10.9299],\n",
            "        [-10.8700]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
            "\n",
            "    New model's output: SequenceClassifierOutput(loss=None, logits=tensor([[ -2.5976,  -3.0268,  -1.3479,  -1.5660,  -3.1606,  -1.8852, -21.8073,\n",
            "          -4.2567,  -2.3831,  -7.3893,  -2.6168,  -5.6001,  -2.4642,  -3.2108,\n",
            "         -21.7702,  -5.2638, -21.3716,  -4.2614,  -5.2135,  -5.3150],\n",
            "        [ -2.4703,  -3.0965,  -1.3549,  -2.0352,  -2.7437,  -2.0568, -21.8894,\n",
            "          -4.3856,  -2.4865,  -7.5653,  -2.6749,  -5.6142,  -2.2101,  -2.9694,\n",
            "         -21.9664,  -5.5138, -21.6562,  -4.1202,  -4.9875,  -5.6576]],\n",
            "       device='cuda:0'), hidden_states=None, attentions=None)\n",
            "    Reference model's output: SequenceClassifierOutput(loss=None, logits=tensor([[ -2.5363,  -3.1314,  -1.2427,  -1.6860,  -3.0604,  -2.0583, -21.7888,\n",
            "          -4.1208,  -2.3355,  -7.4675,  -2.8129,  -5.3138,  -2.3228,  -3.1468,\n",
            "         -21.6905,  -5.4794, -21.6313,  -4.1415,  -5.0822,  -5.6226],\n",
            "        [ -2.5363,  -3.1314,  -1.2427,  -1.6860,  -3.0604,  -2.0583, -21.7888,\n",
            "          -4.1208,  -2.3355,  -7.4675,  -2.8129,  -5.3138,  -2.3228,  -3.1468,\n",
            "         -21.6905,  -5.4794, -21.6313,  -4.1415,  -5.0822,  -5.6226]],\n",
            "       device='cuda:0'), hidden_states=None, attentions=None)\n"
          ]
        }
      ],
      "source": [
        "#We check whether the models were loaded correctly and are ready to inference\n",
        "#For the reward model\n",
        "input = torch.tensor([[2, 2, 1, 3, 4, 1], [3, 2, 1, 3, 4, 4]]).to(device)\n",
        "token_type = torch.tensor([[0, 0, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1]]).to(device)\n",
        "    \n",
        "print(f\"Reward model's output: {rm(input, token_type_ids=token_type, train=False)}\")\n",
        "\n",
        "#For the pieces models\n",
        "input = torch.tensor([[2, 2, 1, 3, 4], [3, 2, 1, 3, 4]]).to(device)\n",
        "token_type = torch.tensor([[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]]).to(device)\n",
        "attention_mask = torch.tensor([[1, 1, 1, 1, 1], [1, 1, 1, 1, 1]]).to(device)\n",
        "with torch.no_grad():\n",
        "    print(f\"\"\"\n",
        "    New model's output: {model_new(input_ids=input, token_type_ids=token_type, attention_mask=attention_mask)}\n",
        "    Reference model's output: {model_ref(input_ids=input, token_type_ids=token_type, attention_mask=attention_mask)}\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "ae6be50c-7707-44dc-89d1-257f568de0e9",
      "metadata": {},
      "outputs": [],
      "source": [
        "#We define the PPO loss as in this hf post (https://huggingface.co/blog/deep-rl-ppo)\n",
        "def PPO_loss(eps:float, model_new, model_ref, reward_model, inp:torch.tensor, verbose:bool=True):\n",
        "    #We first compute the output given by the new and reference model\n",
        "    new_out = nn.functional.softmax(model_new(inp).logits, dim=1)\n",
        "    ref_out = nn.functional.softmax(model_ref(inp).logits, dim=1)\n",
        "\n",
        "    #We compute the ratio of the two outputs\n",
        "    new_out_prob = torch.max(new_out, 1).values\n",
        "    ref_out_prob = torch.max(ref_out, 1).values\n",
        "    ratio = new_out_prob/ref_out_prob\n",
        "\n",
        "    #We format the input for the reward model\n",
        "    max_out = torch.max(new_out, 1).indices\n",
        "    max_out = torch.reshape(max_out, (max_out.size(dim=0), 1))\n",
        "    reward_inp = torch.cat((inp, max_out), 1)\n",
        "    \n",
        "    reward_tokens = torch.zeros_like(reward_inp)\n",
        "    reward_tokens[:, -1] = 1 \n",
        "\n",
        "    #We get the reward assigned by our reward model\n",
        "    reward = reward_model(reward_inp, token_type_ids=reward_tokens, train=False).logits\n",
        "    reward = torch.reshape(reward, (1, reward.size(dim=0)))[0]\n",
        "    \n",
        "    #The entropy is computed to balance explotation and exploration\n",
        "    entropy = torch.tensor([-torch.dot(new_out[i], torch.log2(new_out[i])) for i in range(len(new_out))]).to(device)\n",
        "\n",
        "    #This is the Lclip term presented in the hf post\n",
        "    expression = torch.tensor([(ratio*reward).tolist(), (torch.clamp(ratio, min=1-eps, max=1+eps)*reward).tolist()])\n",
        "\n",
        "    loss = -(torch.sum((torch.min(expression, 0).values.to(device)+entropy)))\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"New model's output: {new_out}\")\n",
        "        print(f\"Reference model's output: {ref_out}\")\n",
        "        print(reward_inp, reward_tokens)\n",
        "        print(f\"The reward for this action and state is: {reward}\")\n",
        "        print(f\"The current entropy is: {entropy}\")\n",
        "        print(f\"The final loss is: {loss}\")\n",
        "\n",
        "    return torch.autograd.Variable(loss, requires_grad = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "fdd7ac83-6c48-4a1a-b9ee-177d4e0fecad",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New model's output: tensor([[7.9668e-02, 3.7430e-02, 2.9483e-01, 1.5748e-01, 3.9525e-02, 1.0632e-01,\n",
            "         2.2878e-10, 1.1194e-02, 9.3476e-02, 5.2020e-04, 3.1839e-02, 3.5076e-03,\n",
            "         7.9789e-02, 4.0205e-02, 3.2621e-10, 3.0629e-03, 3.3976e-10, 1.2742e-02,\n",
            "         5.2590e-03, 3.1531e-03],\n",
            "        [7.3784e-02, 4.0047e-02, 2.6574e-01, 1.5518e-01, 4.1266e-02, 1.3042e-01,\n",
            "         3.9035e-10, 1.3299e-02, 8.7210e-02, 6.0468e-04, 4.8939e-02, 5.1721e-03,\n",
            "         6.8708e-02, 4.7920e-02, 2.7663e-10, 3.2591e-03, 3.2098e-10, 1.1070e-02,\n",
            "         4.6500e-03, 2.7291e-03]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "Reference model's output: tensor([[7.0636e-02, 3.8957e-02, 2.5755e-01, 1.6532e-01, 4.1825e-02, 1.1393e-01,\n",
            "         3.0746e-10, 1.4484e-02, 8.6342e-02, 5.0985e-04, 5.3568e-02, 4.3934e-03,\n",
            "         8.7449e-02, 3.8361e-02, 3.3922e-10, 3.7228e-03, 3.5992e-10, 1.4187e-02,\n",
            "         5.5383e-03, 3.2260e-03],\n",
            "        [7.0636e-02, 3.8957e-02, 2.5755e-01, 1.6532e-01, 4.1825e-02, 1.1393e-01,\n",
            "         3.0746e-10, 1.4484e-02, 8.6342e-02, 5.0985e-04, 5.3568e-02, 4.3934e-03,\n",
            "         8.7449e-02, 3.8361e-02, 3.3922e-10, 3.7228e-03, 3.5992e-10, 1.4187e-02,\n",
            "         5.5383e-03, 3.2260e-03]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[2, 2, 1, 3, 4, 2],\n",
            "        [3, 2, 1, 3, 4, 2]], device='cuda:0') tensor([[0, 0, 0, 0, 0, 1],\n",
            "        [0, 0, 0, 0, 0, 1]], device='cuda:0')\n",
            "The reward for this action and state is: tensor([-10.9646, -10.9441], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "The current entropy is: tensor([3.1697, 3.2438], device='cuda:0')\n",
            "The final loss is: 17.4307804107666\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor(17.4308, device='cuda:0', requires_grad=True)"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Sample of a forward pass using PPO loss function\n",
        "input = torch.tensor([[2, 2, 1, 3, 4], [3, 2, 1, 3, 4]]).to(device)\n",
        "PPO_loss(0.2, model_new, model_ref, rm, input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "8651a02a-301f-4939-9e37-25ef24eba381",
      "metadata": {},
      "outputs": [],
      "source": [
        "#We use Adam optimizer\n",
        "optimizer = torch.optim.Adam(params=model_new.parameters(), lr=3e-07)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "id": "28cf0ca1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 945
        },
        "executionInfo": {
          "elapsed": 65585,
          "status": "error",
          "timestamp": 1701917579957,
          "user": {
            "displayName": "maximo rulli",
            "userId": "05485638369195676564"
          },
          "user_tz": 180
        },
        "id": "28cf0ca1",
        "outputId": "5f025613-f814-470c-e8f0-211029f1823b",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss epoch 0: 13.797601699829102\n",
            "Loss epoch 2: 13.895196914672852\n",
            "Loss epoch 4: 6.327188014984131\n",
            "Loss epoch 6: 16.44588851928711\n",
            "Loss epoch 8: 16.842594146728516\n",
            "Loss epoch 10: 16.496747970581055\n",
            "Loss epoch 12: 9.753284454345703\n",
            "Loss epoch 14: 17.926841735839844\n",
            "Loss epoch 16: 16.47771453857422\n",
            "Loss epoch 18: 14.808004379272461\n",
            "Loss epoch 20: 14.898300170898438\n",
            "Loss epoch 22: 14.517597198486328\n",
            "Loss epoch 24: 11.079215049743652\n",
            "Loss epoch 26: 13.42514419555664\n",
            "Loss epoch 28: 11.610021591186523\n",
            "Loss epoch 30: 18.30743980407715\n",
            "Loss epoch 32: 16.235687255859375\n",
            "Loss epoch 34: 17.545480728149414\n",
            "Loss epoch 36: 14.82377815246582\n",
            "Loss epoch 38: 14.335840225219727\n",
            "Loss epoch 40: 13.071252822875977\n",
            "Loss epoch 42: 15.559148788452148\n",
            "Loss epoch 44: 16.331436157226562\n",
            "Loss epoch 46: 15.651971817016602\n",
            "Loss epoch 48: 12.946578979492188\n",
            "Loss epoch 50: 16.716609954833984\n",
            "Loss epoch 52: 14.01222038269043\n",
            "Loss epoch 54: 14.119315147399902\n",
            "Loss epoch 56: 14.016378402709961\n",
            "Loss epoch 58: 13.211669921875\n",
            "Loss epoch 60: 15.210593223571777\n",
            "Loss epoch 62: 14.45598030090332\n",
            "Loss epoch 64: 12.991795539855957\n",
            "Loss epoch 66: 14.236344337463379\n",
            "Loss epoch 68: 16.49896240234375\n",
            "Loss epoch 70: 12.630763053894043\n",
            "Loss epoch 72: 13.547811508178711\n",
            "Loss epoch 74: 11.229606628417969\n",
            "Loss epoch 76: 15.101836204528809\n",
            "Loss epoch 78: 12.344833374023438\n",
            "Loss epoch 80: 12.669599533081055\n",
            "Loss epoch 82: 14.635653495788574\n",
            "Loss epoch 84: 19.510974884033203\n",
            "Loss epoch 86: 13.24297046661377\n",
            "Loss epoch 88: 2.1788330078125\n",
            "Loss epoch 90: 15.30033016204834\n",
            "Loss epoch 92: 14.567928314208984\n",
            "Loss epoch 94: 14.118484497070312\n",
            "Loss epoch 96: 14.957006454467773\n",
            "Loss epoch 98: 14.83686637878418\n",
            "Loss epoch 100: 17.87956428527832\n",
            "Loss epoch 102: 16.216814041137695\n",
            "Loss epoch 104: 13.19780158996582\n",
            "Loss epoch 106: 13.104241371154785\n",
            "Loss epoch 108: 18.788572311401367\n",
            "Loss epoch 110: 16.711658477783203\n",
            "Loss epoch 112: 16.363605499267578\n",
            "Loss epoch 114: 14.859207153320312\n",
            "Loss epoch 116: 16.87913703918457\n",
            "Loss epoch 118: 13.548423767089844\n",
            "Loss epoch 120: 15.453329086303711\n",
            "Loss epoch 122: 15.123361587524414\n",
            "Loss epoch 124: 16.289478302001953\n",
            "Loss epoch 126: 13.774688720703125\n",
            "Loss epoch 128: 13.253576278686523\n",
            "Loss epoch 130: 11.96531867980957\n",
            "Loss epoch 132: 17.460018157958984\n",
            "Loss epoch 134: 17.538402557373047\n",
            "Loss epoch 136: 19.315876007080078\n",
            "Loss epoch 138: 15.98391342163086\n",
            "Loss epoch 140: 13.684885025024414\n",
            "Loss epoch 142: 9.818129539489746\n",
            "Loss epoch 144: 13.448629379272461\n",
            "Loss epoch 146: 14.384418487548828\n",
            "Loss epoch 148: 13.012551307678223\n",
            "Loss epoch 150: 16.089740753173828\n",
            "Loss epoch 152: 14.465920448303223\n",
            "Loss epoch 154: 12.945862770080566\n",
            "Loss epoch 156: 14.960589408874512\n",
            "Loss epoch 158: 12.228316307067871\n",
            "Loss epoch 160: 17.374683380126953\n",
            "Loss epoch 162: 16.0129451751709\n",
            "Loss epoch 164: 9.276747703552246\n",
            "Loss epoch 166: 18.14967155456543\n",
            "Loss epoch 168: 16.720081329345703\n",
            "Loss epoch 170: 13.776130676269531\n",
            "Loss epoch 172: 10.448481559753418\n",
            "Loss epoch 174: 13.809101104736328\n",
            "Loss epoch 176: -0.31818103790283203\n",
            "Loss epoch 178: 9.149471282958984\n",
            "Loss epoch 180: 14.122180938720703\n",
            "Loss epoch 182: 14.85767936706543\n",
            "Loss epoch 184: 15.459125518798828\n",
            "Loss epoch 186: 14.999571800231934\n",
            "Loss epoch 188: 12.981555938720703\n",
            "Loss epoch 190: 16.83473777770996\n",
            "Loss epoch 192: 11.515523910522461\n",
            "Loss epoch 194: 15.405672073364258\n",
            "Loss epoch 196: 13.3035306930542\n",
            "Loss epoch 198: 14.465532302856445\n",
            "Loss epoch 200: 17.6898193359375\n",
            "Loss epoch 202: 18.887426376342773\n",
            "Loss epoch 204: 16.67526626586914\n",
            "Loss epoch 206: 14.605533599853516\n",
            "Loss epoch 208: 12.956762313842773\n",
            "Loss epoch 210: 15.501449584960938\n",
            "Loss epoch 212: 16.98320770263672\n",
            "Loss epoch 214: 13.290348052978516\n",
            "Loss epoch 216: 14.693986892700195\n",
            "Loss epoch 218: 14.384698867797852\n",
            "Loss epoch 220: 14.907366752624512\n",
            "Loss epoch 222: 16.986530303955078\n",
            "Loss epoch 224: 11.307732582092285\n",
            "Loss epoch 226: 13.817444801330566\n",
            "Loss epoch 228: 13.652629852294922\n",
            "Loss epoch 230: 14.427349090576172\n",
            "Loss epoch 232: 12.312808990478516\n",
            "Loss epoch 234: 17.84510612487793\n",
            "Loss epoch 236: 18.30032730102539\n",
            "Loss epoch 238: 14.555001258850098\n",
            "Loss epoch 240: 13.966310501098633\n",
            "Loss epoch 242: 14.627197265625\n",
            "Loss epoch 244: 14.132963180541992\n",
            "Loss epoch 246: 12.37124252319336\n",
            "Loss epoch 248: 14.425240516662598\n",
            "Loss epoch 250: 12.072582244873047\n",
            "Loss epoch 252: 16.25287437438965\n",
            "Loss epoch 254: 13.006965637207031\n",
            "Loss epoch 256: 16.594867706298828\n",
            "Loss epoch 258: 10.922284126281738\n",
            "Loss epoch 260: 15.031326293945312\n",
            "Loss epoch 262: 9.284597396850586\n",
            "Loss epoch 264: 16.49222183227539\n",
            "Loss epoch 266: 14.65077018737793\n",
            "Loss epoch 268: 15.23366928100586\n",
            "Loss epoch 270: 13.497488021850586\n",
            "Loss epoch 272: 16.103498458862305\n",
            "Loss epoch 274: 10.565522193908691\n",
            "Loss epoch 276: 12.953462600708008\n",
            "Loss epoch 278: 13.307361602783203\n",
            "Loss epoch 280: 14.285881042480469\n",
            "Loss epoch 282: 12.664196968078613\n",
            "Loss epoch 284: 16.038490295410156\n",
            "Loss epoch 286: 13.991935729980469\n",
            "Loss epoch 288: 16.0698299407959\n",
            "Loss epoch 290: 12.395313262939453\n",
            "Loss epoch 292: 13.984580993652344\n",
            "Loss epoch 294: 12.83596420288086\n",
            "Loss epoch 296: 13.2138671875\n",
            "Loss epoch 298: 17.086225509643555\n",
            "Loss epoch 300: 15.036739349365234\n",
            "Loss epoch 302: 13.676218032836914\n",
            "Loss epoch 304: 15.472482681274414\n",
            "Loss epoch 306: 11.2576904296875\n",
            "Loss epoch 308: 18.18280029296875\n",
            "Loss epoch 310: 14.173084259033203\n",
            "Loss epoch 312: 14.275691032409668\n",
            "Loss epoch 314: 11.430294036865234\n",
            "Loss epoch 316: 15.591062545776367\n",
            "Loss epoch 318: 14.556640625\n",
            "Loss epoch 320: 13.95732593536377\n",
            "Loss epoch 322: 16.529695510864258\n",
            "Loss epoch 324: 18.72515106201172\n",
            "Loss epoch 326: 11.232826232910156\n",
            "Loss epoch 328: 16.92231559753418\n",
            "Loss epoch 330: 12.935874938964844\n",
            "Loss epoch 332: 15.427366256713867\n",
            "Loss epoch 334: 16.54582405090332\n",
            "Loss epoch 336: 16.699626922607422\n",
            "Loss epoch 338: 17.57662010192871\n",
            "Loss epoch 340: 15.92742919921875\n",
            "Loss epoch 342: 13.85416030883789\n",
            "Loss epoch 344: 14.364677429199219\n",
            "Loss epoch 346: 14.875101089477539\n",
            "Loss epoch 348: 13.915590286254883\n",
            "Loss epoch 350: 15.119548797607422\n",
            "Loss epoch 352: 14.41091251373291\n",
            "Loss epoch 354: 15.596384048461914\n",
            "Loss epoch 356: 14.107818603515625\n",
            "Loss epoch 358: 14.231781959533691\n",
            "Loss epoch 360: 13.469622611999512\n",
            "Loss epoch 362: 12.728775978088379\n",
            "Loss epoch 364: 17.87898826599121\n",
            "Loss epoch 366: 20.75249481201172\n",
            "Loss epoch 368: 16.945842742919922\n",
            "Loss epoch 370: 18.46277618408203\n",
            "Loss epoch 372: 13.513967514038086\n",
            "Loss epoch 374: 16.634807586669922\n",
            "Loss epoch 376: 11.703371047973633\n",
            "Loss epoch 378: 15.233057022094727\n",
            "Loss epoch 380: 3.1147563457489014\n",
            "Loss epoch 382: 14.045635223388672\n",
            "Loss epoch 384: 16.803882598876953\n",
            "Loss epoch 386: 13.38526725769043\n",
            "Loss epoch 388: 12.976297378540039\n",
            "Loss epoch 390: 14.928091049194336\n",
            "Loss epoch 392: 17.658309936523438\n",
            "Loss epoch 394: 12.0464506149292\n",
            "Loss epoch 396: 14.091184616088867\n",
            "Loss epoch 398: 14.561342239379883\n",
            "Loss epoch 400: 14.640499114990234\n",
            "Loss epoch 402: 11.822525024414062\n",
            "Loss epoch 404: 17.068965911865234\n",
            "Loss epoch 406: 16.798662185668945\n",
            "Loss epoch 408: 13.91385269165039\n",
            "Loss epoch 410: 19.37285804748535\n",
            "Loss epoch 412: 11.088528633117676\n",
            "Loss epoch 414: 16.56705093383789\n",
            "Loss epoch 416: 13.690627098083496\n",
            "Loss epoch 418: 16.680538177490234\n",
            "Loss epoch 420: 11.915233612060547\n",
            "Loss epoch 422: 12.04611587524414\n",
            "Loss epoch 424: 14.123029708862305\n",
            "Loss epoch 426: 11.891560554504395\n",
            "Loss epoch 428: 11.260232925415039\n",
            "Loss epoch 430: 17.212127685546875\n",
            "Loss epoch 432: 12.661211013793945\n",
            "Loss epoch 434: 16.662887573242188\n",
            "Loss epoch 436: 0.3968343734741211\n",
            "Loss epoch 438: 10.577266693115234\n",
            "Loss epoch 440: 14.28061294555664\n",
            "Loss epoch 442: 13.468523979187012\n",
            "Loss epoch 444: 13.912940979003906\n",
            "Loss epoch 446: 11.633499145507812\n",
            "Loss epoch 448: 16.74422836303711\n",
            "Loss epoch 450: 10.638729095458984\n",
            "Loss epoch 452: 16.56732177734375\n",
            "Loss epoch 454: 18.121082305908203\n",
            "Loss epoch 456: 18.554977416992188\n",
            "Loss epoch 458: 18.60163116455078\n",
            "Loss epoch 460: 16.26868438720703\n",
            "Loss epoch 462: 17.67218780517578\n",
            "Loss epoch 464: 11.754344940185547\n",
            "Loss epoch 466: 13.290092468261719\n",
            "Loss epoch 468: 16.03112030029297\n",
            "Loss epoch 470: 18.34939956665039\n",
            "Loss epoch 472: 15.420757293701172\n",
            "Loss epoch 474: 14.922466278076172\n",
            "Loss epoch 476: 14.022700309753418\n",
            "Loss epoch 478: 18.964977264404297\n",
            "Loss epoch 480: 15.12221908569336\n",
            "Loss epoch 482: 17.175992965698242\n",
            "Loss epoch 484: 13.865741729736328\n",
            "Loss epoch 486: 14.44016170501709\n",
            "Loss epoch 488: 16.951087951660156\n",
            "Loss epoch 490: 15.50342082977295\n",
            "Loss epoch 492: 10.836645126342773\n",
            "Loss epoch 494: 16.27081298828125\n",
            "Loss epoch 496: 16.570844650268555\n",
            "Loss epoch 498: 12.339068412780762\n",
            "Loss epoch 500: 18.063434600830078\n",
            "Loss epoch 502: 16.692501068115234\n",
            "Loss epoch 504: 14.333324432373047\n",
            "Loss epoch 506: 14.721992492675781\n",
            "Loss epoch 508: 13.92238998413086\n",
            "Loss epoch 510: 16.254837036132812\n",
            "Loss epoch 512: 17.484329223632812\n",
            "Loss epoch 514: 14.612862586975098\n",
            "Loss epoch 516: 18.269241333007812\n",
            "Loss epoch 518: 17.413070678710938\n",
            "Loss epoch 520: 14.113845825195312\n",
            "Loss epoch 522: 11.46307373046875\n",
            "Loss epoch 524: 14.376141548156738\n",
            "Loss epoch 526: 11.480712890625\n",
            "Loss epoch 528: 18.155014038085938\n",
            "Loss epoch 530: 14.664854049682617\n",
            "Loss epoch 532: 17.677169799804688\n",
            "Loss epoch 534: 14.19846248626709\n",
            "Loss epoch 536: 21.228572845458984\n",
            "Loss epoch 538: 13.381509780883789\n",
            "Loss epoch 540: 13.870163917541504\n",
            "Loss epoch 542: 17.352699279785156\n",
            "Loss epoch 544: 17.481037139892578\n",
            "Loss epoch 546: 15.22555923461914\n",
            "Loss epoch 548: 15.458620071411133\n",
            "Loss epoch 550: 11.792594909667969\n",
            "Loss epoch 552: 16.753631591796875\n",
            "Loss epoch 554: 17.685396194458008\n",
            "Loss epoch 556: 12.338111877441406\n",
            "Loss epoch 558: 18.897140502929688\n",
            "Loss epoch 560: 12.086511611938477\n",
            "Loss epoch 562: 14.867437362670898\n",
            "Loss epoch 564: 12.555130004882812\n",
            "Loss epoch 566: 14.47808837890625\n",
            "Loss epoch 568: 15.586418151855469\n",
            "Loss epoch 570: 14.843828201293945\n",
            "Loss epoch 572: 13.362424850463867\n",
            "Loss epoch 574: 13.127470016479492\n",
            "Loss epoch 576: 11.602646827697754\n",
            "Loss epoch 578: 15.161214828491211\n",
            "Loss epoch 580: 15.705015182495117\n",
            "Loss epoch 582: 18.817033767700195\n",
            "Loss epoch 584: 13.30667495727539\n",
            "Loss epoch 586: 15.942061424255371\n",
            "Loss epoch 588: 15.919021606445312\n",
            "Loss epoch 590: 12.12075424194336\n",
            "Loss epoch 592: 18.495527267456055\n",
            "Loss epoch 594: 13.525140762329102\n",
            "Loss epoch 596: 16.000320434570312\n",
            "Loss epoch 598: 11.834929466247559\n",
            "Loss epoch 600: 16.84882354736328\n",
            "Loss epoch 602: 13.17348861694336\n",
            "Loss epoch 604: 15.671368598937988\n",
            "Loss epoch 606: 12.107667922973633\n",
            "Loss epoch 608: 15.321914672851562\n",
            "Loss epoch 610: 16.634145736694336\n",
            "Loss epoch 612: 16.985652923583984\n",
            "Loss epoch 614: 13.913798332214355\n",
            "Loss epoch 616: 19.685588836669922\n",
            "Loss epoch 618: 15.903741836547852\n",
            "Loss epoch 620: 13.678756713867188\n",
            "Loss epoch 622: 13.037386894226074\n",
            "Loss epoch 624: 13.944744110107422\n",
            "Loss epoch 626: -0.5142664909362793\n",
            "Loss epoch 628: 14.213540077209473\n",
            "Loss epoch 630: 12.924407958984375\n",
            "Loss epoch 632: 14.369394302368164\n",
            "Loss epoch 634: 17.09762954711914\n",
            "Loss epoch 636: 16.19940948486328\n",
            "Loss epoch 638: 14.535688400268555\n",
            "Loss epoch 640: 16.43535041809082\n",
            "Loss epoch 642: 20.540977478027344\n",
            "Loss epoch 644: 16.11760902404785\n",
            "Loss epoch 646: 13.112676620483398\n",
            "Loss epoch 648: 15.824850082397461\n",
            "Loss epoch 650: 16.014312744140625\n",
            "Loss epoch 652: 15.074148178100586\n",
            "Loss epoch 654: 14.277050971984863\n",
            "Loss epoch 656: 12.829631805419922\n",
            "Loss epoch 658: 17.420198440551758\n",
            "Loss epoch 660: 15.404078483581543\n",
            "Loss epoch 662: 15.381454467773438\n",
            "Loss epoch 664: 23.12881851196289\n",
            "Loss epoch 666: 15.327072143554688\n",
            "Loss epoch 668: 0.3963441848754883\n",
            "Loss epoch 670: 15.06358528137207\n",
            "Loss epoch 672: 12.977797508239746\n",
            "Loss epoch 674: 16.06788444519043\n",
            "Loss epoch 676: 16.4991397857666\n",
            "Loss epoch 678: 11.918127059936523\n",
            "Loss epoch 680: 14.965706825256348\n",
            "Loss epoch 682: 10.198495864868164\n",
            "Loss epoch 684: 12.748882293701172\n",
            "Loss epoch 686: 12.368671417236328\n",
            "Loss epoch 688: 16.517528533935547\n",
            "Loss epoch 690: 11.929533004760742\n",
            "Loss epoch 692: 12.353857040405273\n",
            "Loss epoch 694: 13.432489395141602\n",
            "Loss epoch 696: 15.532942771911621\n",
            "Loss epoch 698: 14.722658157348633\n",
            "Loss epoch 700: 11.570549011230469\n",
            "Loss epoch 702: 15.775784492492676\n",
            "Loss epoch 704: 17.097919464111328\n",
            "Loss epoch 706: 16.061695098876953\n",
            "Loss epoch 708: 12.530801773071289\n",
            "Loss epoch 710: 13.111398696899414\n",
            "Loss epoch 712: 12.085357666015625\n",
            "Loss epoch 714: 15.459487915039062\n",
            "Loss epoch 716: 12.182960510253906\n",
            "Loss epoch 718: 13.01551628112793\n",
            "Loss epoch 720: -1.5769052505493164\n",
            "Loss epoch 722: 15.458922386169434\n",
            "Loss epoch 724: 13.496199607849121\n",
            "Loss epoch 726: 12.48043441772461\n",
            "Loss epoch 728: 13.85068130493164\n",
            "Loss epoch 730: 17.868120193481445\n",
            "Loss epoch 732: 10.072978973388672\n",
            "Loss epoch 734: 13.308693885803223\n",
            "Loss epoch 736: 13.207969665527344\n",
            "Loss epoch 738: 13.140889167785645\n",
            "Loss epoch 740: 17.25236701965332\n",
            "Loss epoch 742: 12.789741516113281\n",
            "Loss epoch 744: 17.547880172729492\n",
            "Loss epoch 746: 16.92311668395996\n",
            "Loss epoch 748: 13.232508659362793\n",
            "Loss epoch 750: 14.846957206726074\n",
            "Loss epoch 752: 16.036449432373047\n",
            "Loss epoch 754: 7.754345893859863\n",
            "Loss epoch 756: 15.242681503295898\n",
            "Loss epoch 758: 13.014711380004883\n",
            "Loss epoch 760: 15.640676498413086\n",
            "Loss epoch 762: 13.044336318969727\n",
            "Loss epoch 764: 13.819221496582031\n",
            "Loss epoch 766: 13.469520568847656\n",
            "Loss epoch 768: 15.697786331176758\n",
            "Loss epoch 770: 12.627474784851074\n",
            "Loss epoch 772: 14.277053833007812\n",
            "Loss epoch 774: 20.725948333740234\n",
            "Loss epoch 776: 12.946568489074707\n",
            "Loss epoch 778: 11.592117309570312\n",
            "Loss epoch 780: 3.5145761966705322\n",
            "Loss epoch 782: 14.064672470092773\n",
            "Loss epoch 784: 16.01793098449707\n",
            "Loss epoch 786: 16.199417114257812\n",
            "Loss epoch 788: 10.680084228515625\n",
            "Loss epoch 790: 11.584498405456543\n",
            "Loss epoch 792: 14.341621398925781\n",
            "Loss epoch 794: 17.671123504638672\n",
            "Loss epoch 796: 14.614004135131836\n",
            "Loss epoch 798: 11.427328109741211\n",
            "Loss epoch 800: 13.251253128051758\n",
            "Loss epoch 802: 14.52621841430664\n",
            "Loss epoch 804: 10.975418090820312\n",
            "Loss epoch 806: 16.63315200805664\n",
            "Loss epoch 808: 14.191644668579102\n",
            "Loss epoch 810: 7.601484298706055\n",
            "Loss epoch 812: 17.122344970703125\n",
            "Loss epoch 814: 14.29049301147461\n",
            "Loss epoch 816: 13.923334121704102\n",
            "Loss epoch 818: 17.118606567382812\n",
            "Loss epoch 820: 18.473392486572266\n",
            "Loss epoch 822: 18.208415985107422\n",
            "Loss epoch 824: 14.861676216125488\n",
            "Loss epoch 826: 10.905704498291016\n",
            "Loss epoch 828: 13.236271858215332\n",
            "Loss epoch 830: 16.51500129699707\n",
            "Loss epoch 832: 14.316093444824219\n",
            "Loss epoch 834: 14.501051902770996\n",
            "Loss epoch 836: 13.879919052124023\n",
            "Loss epoch 838: 14.244146347045898\n",
            "Loss epoch 840: 18.20016098022461\n",
            "Loss epoch 842: 15.595917701721191\n",
            "Loss epoch 844: 16.766529083251953\n",
            "Loss epoch 846: 14.488077163696289\n",
            "Loss epoch 848: 16.80666732788086\n",
            "Loss epoch 850: 14.80148696899414\n",
            "Loss epoch 852: 14.921384811401367\n",
            "Loss epoch 854: 14.52846908569336\n",
            "Loss epoch 856: 14.905900001525879\n",
            "Loss epoch 858: 15.362227439880371\n",
            "Loss epoch 860: 17.743886947631836\n",
            "Loss epoch 862: 17.29486846923828\n",
            "Loss epoch 864: 15.857725143432617\n",
            "Loss epoch 866: 12.151817321777344\n",
            "Loss epoch 868: 16.842924118041992\n",
            "Loss epoch 870: 17.683937072753906\n",
            "Loss epoch 872: 19.576889038085938\n",
            "Loss epoch 874: 18.20224380493164\n",
            "Loss epoch 876: 17.81479263305664\n",
            "Loss epoch 878: 17.457590103149414\n",
            "Loss epoch 880: 13.762802124023438\n",
            "Loss epoch 882: 17.198284149169922\n",
            "Loss epoch 884: 12.442663192749023\n",
            "Loss epoch 886: 15.977472305297852\n",
            "Loss epoch 888: 14.976229667663574\n",
            "Loss epoch 890: 10.366693496704102\n",
            "Loss epoch 892: 14.819839477539062\n",
            "Loss epoch 894: 13.395586967468262\n",
            "Loss epoch 896: 14.647032737731934\n",
            "Loss epoch 898: 13.114892959594727\n",
            "Loss epoch 900: 0.38805103302001953\n",
            "Loss epoch 902: 12.308287620544434\n",
            "Loss epoch 904: 7.998333930969238\n",
            "Loss epoch 906: 17.10133171081543\n",
            "Loss epoch 908: 13.313701629638672\n",
            "Loss epoch 910: 15.034229278564453\n",
            "Loss epoch 912: 14.730915069580078\n",
            "Loss epoch 914: 13.729103088378906\n",
            "Loss epoch 916: 19.4787654876709\n",
            "Loss epoch 918: 20.0858154296875\n",
            "Loss epoch 920: 11.054378509521484\n",
            "Loss epoch 922: 21.717124938964844\n",
            "Loss epoch 924: 16.738788604736328\n",
            "Loss epoch 926: 17.908790588378906\n",
            "Loss epoch 928: 5.926691055297852\n",
            "Loss epoch 930: 17.26057243347168\n",
            "Loss epoch 932: 15.642248153686523\n",
            "Loss epoch 934: 16.13558578491211\n",
            "Loss epoch 936: 13.896764755249023\n",
            "Loss epoch 938: 16.284183502197266\n",
            "Loss epoch 940: 6.8084282875061035\n",
            "Loss epoch 942: 15.722453117370605\n",
            "Loss epoch 944: 15.819403648376465\n",
            "Loss epoch 946: 14.515755653381348\n",
            "Loss epoch 948: 9.449827194213867\n",
            "Loss epoch 950: 16.227108001708984\n",
            "Loss epoch 952: 18.76413917541504\n",
            "Loss epoch 954: 14.474345207214355\n",
            "Loss epoch 956: 16.574710845947266\n",
            "Loss epoch 958: 17.547937393188477\n",
            "Loss epoch 960: 16.947580337524414\n",
            "Loss epoch 962: 10.610876083374023\n",
            "Loss epoch 964: 11.21669864654541\n",
            "Loss epoch 966: 12.16337776184082\n",
            "Loss epoch 968: 15.92347240447998\n",
            "Loss epoch 970: 12.85535717010498\n",
            "Loss epoch 972: 13.872055053710938\n",
            "Loss epoch 974: 14.923135757446289\n",
            "Loss epoch 976: 15.414037704467773\n",
            "Loss epoch 978: 14.229738235473633\n",
            "Loss epoch 980: 15.73348617553711\n",
            "Loss epoch 982: 14.569714546203613\n",
            "Loss epoch 984: 16.87566566467285\n",
            "Loss epoch 986: 13.485715866088867\n",
            "Loss epoch 988: 17.71627426147461\n",
            "Loss epoch 990: 14.265338897705078\n",
            "Loss epoch 992: 14.667860984802246\n",
            "Loss epoch 994: 14.431924819946289\n",
            "Loss epoch 996: 14.529830932617188\n",
            "Loss epoch 998: 13.975892066955566\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 1000\n",
        "BATCH_SIZE = 2\n",
        "MAX_LEN = 15\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    # Generate random sequence as training example\n",
        "    inp_len = np.random.randint(2, MAX_LEN+1)\n",
        "    input = torch.tensor(np.random.randint(0, 20, size=(BATCH_SIZE, inp_len))).to(device)\n",
        "\n",
        "    # Compute loss (does forward pass also)\n",
        "    loss = PPO_loss(0.2, model_new, model_ref, rm, input, verbose=False)\n",
        "\n",
        "    if epoch%50 == 0:\n",
        "        #print(f\"Correct reward: {out[0].logits} Wrong reward: {out[1].logits}\")\n",
        "        print(f\"Loss epoch {epoch}: {loss}\")\n",
        "\n",
        "    # Backpropagate\n",
        "    loss.backward()\n",
        "\n",
        "    # Update weights\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "id": "57b45919-9af1-49cc-9b71-dee14e8d95d6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New model's output: tensor([[6.5049e-02, 2.6806e-02, 2.2435e-01, 1.3317e-01, 3.2102e-02, 1.5782e-01,\n",
            "         3.2575e-10, 1.7956e-02, 8.3287e-02, 5.2001e-04, 6.4405e-02, 4.4030e-03,\n",
            "         1.2668e-01, 3.7096e-02, 3.2886e-10, 3.5549e-03, 3.1342e-10, 1.3814e-02,\n",
            "         5.7693e-03, 3.2172e-03],\n",
            "        [8.0917e-02, 4.3634e-02, 2.6163e-01, 1.2973e-01, 3.8722e-02, 1.1113e-01,\n",
            "         2.6086e-10, 1.1044e-02, 9.9301e-02, 5.3719e-04, 4.9703e-02, 4.7165e-03,\n",
            "         9.8419e-02, 4.5927e-02, 3.0808e-10, 3.3190e-03, 2.6936e-10, 1.2594e-02,\n",
            "         5.3902e-03, 3.2824e-03]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "Reference model's output: tensor([[7.0636e-02, 3.8957e-02, 2.5755e-01, 1.6532e-01, 4.1825e-02, 1.1393e-01,\n",
            "         3.0746e-10, 1.4484e-02, 8.6342e-02, 5.0985e-04, 5.3568e-02, 4.3934e-03,\n",
            "         8.7449e-02, 3.8361e-02, 3.3922e-10, 3.7228e-03, 3.5992e-10, 1.4187e-02,\n",
            "         5.5383e-03, 3.2260e-03],\n",
            "        [7.0636e-02, 3.8957e-02, 2.5755e-01, 1.6532e-01, 4.1825e-02, 1.1393e-01,\n",
            "         3.0746e-10, 1.4484e-02, 8.6342e-02, 5.0985e-04, 5.3568e-02, 4.3934e-03,\n",
            "         8.7449e-02, 3.8361e-02, 3.3922e-10, 3.7228e-03, 3.5992e-10, 1.4187e-02,\n",
            "         5.5383e-03, 3.2260e-03]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "tensor([[2, 2, 1, 3, 4, 2],\n",
            "        [3, 2, 1, 3, 4, 2]], device='cuda:0') tensor([[0, 0, 0, 0, 0, 1],\n",
            "        [0, 0, 0, 0, 0, 1]], device='cuda:0')\n",
            "The reward for this action and state is: tensor([-10.9646, -10.9441], device='cuda:0', grad_fn=<SelectBackward0>)\n",
            "The current entropy is: tensor([3.2826, 3.2809], device='cuda:0')\n",
            "The final loss is: 14.105611801147461\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor(14.1056, device='cuda:0', requires_grad=True)"
            ]
          },
          "execution_count": 128,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Test that reward and reference model are still the same\n",
        "input = torch.tensor([[2, 2, 1, 3, 4], [3, 2, 1, 3, 4]]).to(device)\n",
        "PPO_loss(0.2, model_new, model_ref, rm, input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cec8f134-3423-420d-9e4c-5f228128fb47",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}