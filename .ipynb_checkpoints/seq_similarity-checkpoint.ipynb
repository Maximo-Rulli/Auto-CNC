{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximo/miniconda3/envs/ml/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "from utils import PROD_TOK, PIEZ_TOK, preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pr = AutoModelForSequenceClassification.from_pretrained(\"VCNC/bert_3\").to(device)\n",
    "model_pi = AutoModelForSequenceClassification.from_pretrained(\"VCNC/bert_piezas\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=3072, out_features=768, bias=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pi.bert.encoder.layer[11].output.dense #Demostración de como acceder a una capa en específico del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para extraer rasgos intermedios del modelo lo que hay que hacer es crear un gancho (hook)\n",
    "def get_features(name):\n",
    "    def hook(model, input, output):\n",
    "        features[name] = output.detach()\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7fe47735f340>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pi.bert.encoder.layer[11].output.dense.register_forward_hook(get_features('feats')) #Registramos el gancho en el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp1 = torch.tensor([[1, 2, 4, 1, 6, 7]]).to(device)\n",
    "inp2 = torch.tensor([[4, 2, 3, 5, 7]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ -2.4952,  -3.1932,  -1.2001,  -1.7882,  -3.1423,  -1.9941, -20.5563,\n",
       "          -4.1423,  -2.4272,  -7.4258,  -2.7523,  -5.4735,  -2.3379,  -3.1901,\n",
       "         -20.5552,  -5.5140, -20.5605,  -4.0876,  -5.1690,  -5.4560]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = {} #Creamos un diccionario donde se van a almacenar los rasgos intermedios que el gancho obtenga\n",
    "model_pi(input_ids=inp1) #Hacemos una llamada al modelo (notese como no hace falta guardar la salida del mismo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features['feats'][0][0]) #Accedemos al vector de features que contiene la salida intermedia, notese como la longitud es de 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_distance(sent1:torch.tensor, sent2:torch.tensor, model):\n",
    "    global features\n",
    "    features = {}\n",
    "    model(input_ids=sent1)\n",
    "    feat1 = features['feats'][0][0]\n",
    "    features = {}\n",
    "    model(input_ids=sent2)\n",
    "    feat2 = features['feats'][0][0]\n",
    "    return np.linalg.norm(feat1.cpu()-feat2.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3077558e-05"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_distance(inp1, inp2, model_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Metemos todo junto en una sola función\n",
    "def sequence_similarity(inp1:list[str], inp2:list[str], model:str, tokens:dict, device:torch.device):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model).to(device)\n",
    "    def get_features(name):\n",
    "        def hook(model, input, output):\n",
    "            features[name] = output.detach()\n",
    "        return hook\n",
    "    model.bert.encoder.layer[11].output.dense.register_forward_hook(get_features('feats'))\n",
    "    sent1 = preprocess(inp1, tokens).to(device)\n",
    "    sent2 = preprocess(inp2, tokens).to(device)\n",
    "    return sentence_distance(sent1, sent2, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp1 = ['1800-ARK', '1200 TAPA 6','1200 CINCO ROSCAS','1200 DADO VUELTA','1600','1800', '1200', '1200' 'other']\n",
    "inp2 = ['1800-ARK', '1200 TAPA 6','1200 CINCO ROSCAS','1200 DADO VUELTA','1600','1800', '1200', '1200' 'other']\n",
    "model = 'VCNC/bert_3'\n",
    "tokens = PROD_TOK\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sequence_similarity(inp1, inp2, model, tokens, device) #Si la salida da 0 significa que funcionó, ya que ambos inputs son exactamente iguales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función sequence_similarity junto con la función sentence_distance están implementadas en utils.py para ser importadas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
